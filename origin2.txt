Natalie Paquette spends her time thinking about how to grow an extra dimension. Start with little circles, scattered across every point in space and time-a curlicue dimension, looped back onto itself. Then shrink those circles down, smaller and smaller, tightening the loop, until a curious transformation occurs: the dimension stops seeming tiny and instead becomes enormous, like when you realize something that looks small and nearby is actually huge and distant. We're shrinking a spatial direction, Paquette says. But when we try to shrink it past a certain point, a new, large spatial direction emerges instead. Paquette, a theoretical physicist at the University of Washington, is not alone in thinking about this strange kind of dimensional transmutation. A growing number of physicists, working in different areas of the discipline with different approaches, are increasingly converging on a profound idea: space-and perhaps even time-is not fundamental. Instead space and time may be emergent: they could arise from the structure and behavior of more basic components of nature. At the deepest level of reality, questions like "Where?" and "When?" simply may not have answers at all. We have a lot of hints from physics that spacetime as we understand it isn't the fundamental thing, Paquette says. These radical notions come from the latest twists in the century-long hunt for a theory of quantum gravity. Physicists' best theory of gravity is general relativity, Albert Einstein's famous conception of how matter warps space and time. Their best theory of everything else is quantum physics, which is astonishingly accurate when it comes to the properties of matter, energy and subatomic particles. Both theories have easily passed all the tests physicists have been able to devise for the past century. Put them together, one might think, and you would have a theory of everything. But the two theories don't play nicely. Ask general relativity what happens in the context of quantum physics, and you'll get contradictory answers, with untamed infinities breaking loose across your calculations. Nature knows how to apply gravity in quantum contexts-it happened in the first moments of the big bang, and it still happens in the hearts of black holes-but we humans are still struggling to understand how the trick is done. Part of the problem lies in the ways the two theories deal with space and time. While quantum physics treats space and time as immutable, general relativity warps them for breakfast. Somehow a theory of quantum gravity would need to reconcile these ideas about space and time. One way to do that would be to eliminate the problem at its source, spacetime itself, by making space and time emerge from something more fundamental. In recent years several different lines of inquiry have all suggested that, at the deepest level of reality, space and time do not exist in the same way that they do in our everyday world. Over the past decade these ideas have radically changed how physicists think about black holes. Now researchers are using these concepts to elucidate the workings of something even more exotic: wormholes-hypothetical tunnel-like connections between distant points in spacetime. These successes have kept alive the hope of an even deeper breakthrough. If spacetime is emergent, then figuring out where it comes from-and how it could arise from anything else-may just be the missing key that finally unlocks the door to a theory of everything. Today the most popular candidate theory of quantum gravity among physicists is string theory. According to this idea, its eponymous strings are the fundamental constituents of matter and energy, giving rise to the myriad fundamental subatomic particles seen at particle accelerators around the world. They are even responsible for gravity-a hypothetical particle that carries the gravitational force, a "graviton," is an inevitable consequence of the theory. But string theory is difficult to understand-it lives in mathematical territory that has taken physicists and mathematicians decades to explore. Much of the theory's structure is still uncharted, expeditions still planned and maps left to be made. Within this new realm, the main technique for navigation is through mathematical dualities-correspondences between one kind of system and another. One example is the duality from the beginning of this article, between tiny dimensions and big ones. Try to cram a dimension down into a little space, and string theory tells you that you will end up with something mathematically identical to a world where that dimension is huge instead. The two situations are the same, according to string theory-you can go back and forth from one to the other freely and use techniques from one situation to understand how the other one works. If you carefully keep track of the fundamental building blocks of the theory," Paquette says, "you can naturally find sometimes that you might grow a new spatial dimension. A similar duality suggests to many string theorists that space itself is emergent. The idea began in 1997, when Juan Maldacena, a physicist at the Institute for Advanced Study, uncovered a duality between a kind of well-understood quantum theory known as a conformal field theory (CFT) and a special kind of spacetime from general relativity known as anti-de Sitter space (AdS). The two seem to be wildly different theories-the CFT has no gravity in it whatsoever, and the AdS space has all of Einstein's theory of gravity thrown in. Yet the same mathematics can describe both worlds. When it was discovered, this AdS/CFT correspondence provided a tangible mathematical link between a quantum theory and a full universe with gravity in it. Curiously, the AdS space in the AdS/CFT correspondence had one more dimension in it than the quantum CFT had. But physicists relished this mismatch because it was a fully worked-out example of another kind of correspondence conceived a few years earlier, from physicists Gerard't Hooft of Utrecht University in the Netherlands and Leonard Susskind of Stanford University, known as the holographic principle. Based on some of the peculiar characteristics of black holes, 't Hooft and Susskind suspected that the properties of a region of space might be fully "encoded" by its boundary. In other words, the two-dimensional surface of a black hole would contain all the information needed to know what was in its three-dimensional interior-like a hologram. I think a lot of people thought we were nuts," Susskind says. Two good physicists gone bad. Similarly, in the AdS/CFT correspondence, the four-dimensional CFT encodes everything about the five-dimensional AdS space it is associated with. In this system, the entire region of spacetime is built out of interactions between the components of the quantum system in the conformal field theory. Maldacena likens this process to reading a novel. If you are telling a story in a book, there are the characters in the book that are doing something," he says. But all there is is a line of text, right. What the characters are doing is inferred from this line of text. The characters in the book would be like the bulk [AdS] theory. And the line of text is the [CFT]. But where does the space in the AdS space come from. If this space is emergent, what is it emerging from. The answer is a special and strangely quantum kind of interaction in the CFT: entanglement, a long-distance connection between objects, instantaneously correlating their behavior in statistically improbable ways. Entanglement famously troubled Einstein, who called it spooky action at a distance. Yet despite its spookiness, entanglement is a core feature of quantum physics. When any two objects interact in quantum mechanics, they generally become entangled and will stay entangled so long as they remain isolated from the rest of the world-no matter how far apart they may travel. In experiments, physicists have maintained entanglement between particles more than 1,000 kilometers apart and even between particles on the ground and others sent to orbiting satellites. In principle, two entangled particles could sustain their connection on opposite sides of the galaxy or the universe. Distance simply does not seem to matter for entanglement, a puzzle that has troubled many physicists for decades. But if space is emergent, entanglement's ability to persist over large distances might not be terribly mysterious-after all, distance is a construct. According to studies of the AdS/CFT correspondence by physicists Shinsei Ryu of Princeton University and Tadashi Takayanagi of Kyoto University, entanglement is what produces distances in the AdS space in the first place. Any two nearby regions of space on the AdS side of the duality correspond to two highly entangled quantum components of the CFT. The more entangled they are, the closer together the regions of space are. In recent years physicists have come to suspect that this relation might apply to our universe as well. What is it that holds the space together and keeps it from falling apart into separate subregions. The answer is the entanglement between two parts of space," Susskind says. The continuity and the connectivity of space owes its existence to quantum-mechanical entanglement. Entanglement, then, may undergird the structure of space itself, forming the warp and weft that give rise to the geometry of the world. If you could somehow destroy the entanglement between two parts [of space], the space would fall apart," Susskind says. It would do the opposite of emerging. It would dis-emerge. If space is made of entanglement, then the puzzle of quantum gravity seems much easier to solve: instead of trying to account for the warping of space in a quantum way, space itself emerges out of a fundamentally quantum phenomenon. Susskind suspects this is why a theory of quantum gravity has been so difficult to find in the first place. I think the reason it never worked very well is because it started with a picture of two different things, [general relativity] and quantum mechanics, and put them together, he says. And I think the point is really that they're much too closely related to pull apart and then put back together again. There's no such thing as gravity without quantum mechanics. Yet accounting for emergent space is only half the job. With space and time so intimately linked in relativity, any account of how space emerges must also explain time. Time must also emerge somehow, says Mark van Raamsdonk, a physicist at the University of British Columbia and a pioneer in the connection between entanglement and spacetime. But this is not well understood and is an active area of research. Another active area, he says, is using models of emergent spacetime to understand wormholes. Previously many physicists had believed that sending objects through a wormhole was impossible, even in theory. But in the past few years physicists working on the AdS/CFT correspondence and similar models have found new ways to construct wormholes. We don't know if we could do that in our universe, van Raamsdonk says. But what we now know is that certain kinds of traversable wormholes are theoretically possible. Two papers-one in 2016 and one in 2018-led to an ongoing flurry of work in the area. But even if traversable wormholes could be built, they would not be much use for space travel. As Susskind points out, you can't go through that wormhole faster than it would take for [light] to go the long way around. If the string theorists are correct, then space is built from quantum entanglement, and time might be as well. But what would that really mean. How can space be made of entanglement between objects unless those objects are themselves somewhere. How can those objects become entangled unless they experience time and change. And what kind of existence could things have without inhabiting a true space and time. These are questions verging on philosophy-and indeed, philosophers of physics are taking them seriously. How the hell could spacetime be the kind of thing that could be emergent. Asks Eleanor Knox, a philosopher of physics at King's College London. Intuitively, she says, that seems impossible. But Knox doesn't think that is a problem. Our intuitions are terrible sometimes, she says. They evolved on the African savanna interacting with macro objects and macro fluids and biological animals and tend not to transfer to the world of quantum mechanics. When it comes to quantum gravity, "Where's the stuff?" and "Where does it live?" aren't the right questions to be asking, Knox concludes. It is certainly true that objects live in places in everyday life. But as Knox and many others point out, that does not mean that space and time have to be fundamental-just that they have to reliably emerge from whatever is fundamental. Consider a liquid, says Christian Wuthrich, a philosopher of physics at the University of Geneva. Ultimately it's elementary particles, like electrons and protons and neutrons or, even more fundamental, quarks and leptons. Do quarks and leptons have liquid properties. That just doesn't make sense, right. Nevertheless, when these fundamental particles come together in sufficient numbers and show a certain behavior together, collective behavior, then they will act in a way that is like a liquid. Space and time, Wuthrich says, could work the same way in string theory and other theories of quantum gravity. Specifically, spacetime might emerge from the materials we usually think of as living in the universe-matter and energy itself. It's not [that] we first have space and time and then we add in some matter, Wuthrich says. Rather something material may be a necessary condition for there to be space and time. That's still a very close connection, but it's just the other way from what you might have thought originally. But there are other ways to interpret the latest findings. The AdS/CFT correspondence is often seen as an example of how spacetime might emerge from a quantum system, but that might not actually be what it shows, according to Alyssa Ney, a philosopher of physics at the University of California, Davis. AdS/CFT gives you this ability to provide a translation manual between facts about the spacetime and facts of the quantum theory, Ney says. That's compatible with the claim that spacetime is emergent, and some quantum theory is fundamental. But the reverse is also true, she says. The correspondence could mean that quantum theory is emergent and spacetime is fundamental-or that neither is fundamental and that there is some even deeper fundamental theory out there. Emergence is a strong claim to make, Ney says, and she is open to the possibility that it is true. But at least just looking at AdS/CFT, I'm still not seeing a clear argument for emergence. An arguably bigger challenge to the string theory picture of emergent spacetime is hidden in plain sight, right in the name of the AdS/CFT correspondence itself. We don't live in anti–de Sitter space, Susskind says. We live in something much closer to de Sitter space. De Sitter space describes an accelerating and expanding universe much like our own. We haven't got the vaguest idea how [holography] applies there, Susskind concludes. Figuring out how to set up this kind of correspondence for a space that more closely resembles the actual universe is one of the most pressing problems for string theorists. I think we're going to be able to understand better how to get into a cosmological version of this, van Raamsdonk says. Finally, there is the news-or lack thereof-from the latest particle accelerators, which have not found any evidence for the extra particles predicted by supersymmetry, an idea that string theory relies on. Supersymmetry dictates that all known particles would have their own superpartners, doubling the number of fundamental particles. But CERN's Large Hadron Collider near Geneva, designed in part to search for superpartners, has seen no sign of them. All of the really precise versions of [emergent spacetime] that we have are in supersymmetric theories, Susskind says. Once you don't have supersymmetry, the ability to mathematically follow the equations just evaporates out of your hands. String theory is not the only idea that suggests spacetime is emergent. String theory has failed to live up to [its] promise as a way to unite gravity and quantum mechanics, says Abhay Ashtekar, a physicist at Pennsylvania State University. The power of string theory now is in providing an extremely rich set of tools, which has been used widely across the whole spectrum of physics. Ashtekar is one of the original pioneers of the most popular alternative to string theory, known as loop quantum gravity. In loop quantum gravity, space and time are not smooth and continuous the way they are in general relativity-instead they are made of discrete components, what Ashtekar calls chunks or atoms of spacetime. These atoms of spacetime are connected in a network, with one- and two-dimensional surfaces joining them together into what practitioners of loop quantum gravity call a spin foam. And despite that foam being limited to two dimensions, it gives rise to our four-dimensional world, with three dimensions of space and one of time. Ashtekar likens it to a piece of clothing. If you look at your shirt, it looks like a two-dimensional surface, he says. If you just take a magnifying glass, you will immediately see that it's all one-dimensional threads. It's just that those threads are so densely packed that for all practical purposes, you can think of the shirt as being a two-dimensional surface. So, similarly, the space around us looks like a three-dimensional continuum. But there is really a crisscross by these [atoms of spacetime]. Although string theory and loop quantum gravity both suggest that spacetime is emergent, the kind of emergence is different in the two theories. String theory suggests that spacetime (or at least space) emerges from the behavior of a seemingly unrelated system, in the form of entanglement. Think of how traffic jams emerge from the collective decisions of individual drivers. The cars are not made of traffic-the cars make the traffic. In loop quantum gravity, on the other hand, the emergence of spacetime is more like a sloping sand dune emerging from the collective motion of sand grains in wind. The smooth familiar spacetime comes from the collective behavior of tiny grains of spacetime; like the dunes, the grains are still sand, even though the chunky crystalline grains do not look or act like the undulating dunes. Despite these differences, both loop quantum gravity and string theory suggest spacetime emerges from some underlying reality. Nor are they the only proposed theories of quantum gravity that point in this direction. Causal set theory, another contender for a theory of quantum gravity, posits that space and time are made of more fundamental components as well. It's really striking that for most of the plausible theories of quantum gravity that we have, in some sense their message is, yeah, general relativistic spacetime isn't in there at the fundamental level, Knox says. People get very excited when different theories of quantum gravity agree on at least something. Modern physics is a victim of its own success. Because quantum physics and general relativity are both so phenomenally accurate, quantum gravity is needed only to describe extreme situations, when enormous masses are stuffed into unfathomably tiny spaces. Those conditions exist in only a few places in nature, such as the center of a black hole-and notably not in physics laboratories, not even the largest and most powerful ones. It would take a particle accelerator the size of a galaxy to directly test the behavior of nature under conditions where quantum gravity reigns. This lack of direct experimental data is a large part of the reason why scientists' search for a theory of quantum gravity has been so long. Faced with the lack of evidence, most physicists have pinned their hopes on the sky. In the earliest moments of the big bang, the entire universe was phenomenally small and dense-a situation that calls for quantum gravity to describe it. And echoes of that era may remain in the sky today. I think our best bet [for testing quantum gravity] is through cosmology, Maldacena says. Maybe something in cosmology that we now think is unpredictable, that maybe can be predicted once we understand the full theory, or some new thing that we didn't even think about. Laboratory experiments may come in handy, however, for testing string theory, at least indirectly. Scientists hope to study the AdS/CFT correspondence not by probing spacetime but by building highly entangled systems of atoms and seeing whether an analogue to spacetime and gravity shows up in their behavior. Such experiments might have some features of gravity, though, perhaps not all the features, Maldacena says. It also depends on exactly what you call gravity. Will we ever know the real nature of space and time. The observational data from the skies may not be forthcoming any time soon. The lab experiments could be a bust. And as philosophers know well, questions about the true nature of space and time are very old indeed. What exists is now all together, one, continuous, said the philosopher Parmenides 2,500 years ago. All is full of what is. Parmenides insisted that time and change were illusions, that everything everywhere was one and the same. His pupil Zeno created famous paradoxes to prove his teacher's point, purporting to show that motion over any distance was impossible. Their work raised the question of whether time and space are somehow illusory, an unsettling prospect that has haunted Western philosophy for over two millennia. The fact that the ancient Greeks asked things like, What is space?' 'What is time?' 'What is change?' and that we still ask versions of these questions today means that they were the right questions to ask, Wuthrich says. It's by thinking about these kinds of questions that we have learned a lot about physics. Humans have tracked time in one way or another in every civilization we have records of, writes physicist Chad Orzel. In his new book A Brief History of Timekeeping (BenBella Books, 2022), Orzel chronicles Neolithic efforts to predict solstices and other astronomical events, the latest atomic clocks that keep time to ever more precise decimals and everything that came in between. He describes the evolution of clocks, from water clocks that timed intervals by how long it took water to flow out of a container to hourglasses filled with sand to the first mechanical and pendulum clocks to our modern era. Each episode is filled with interesting physics and engineering, as well as a glimpse at how different ways of keeping time affected how people lived their lives at various points and places in history. Scientific American talked to Orzel about the coolest clocks in history, the most complicated calendar systems and why we still need to improve the best clocks of today. There's an interesting democratization of time as you go along. The very most ancient monuments are things such as Newgrange in Ireland. It's this massive artificial hill with a passage through the center. Once a year sunlight reaches that central chamber, and that tells you it's the winter solstice. I've been there, and you can put 10 to 12 people in there, maybe. This is an elite thing where only a few people have access to this information. As you start to get things such as water clocks, that's something that individual people can use to time things. They're not superaccurate, but that makes it more accessible. Mechanical clocks make it even better, and then you get public clocks-clocks on church towers with bells that ring out the hours. Everybody starts to have access to time. Mechanical watches start to become reasonably accurate and reasonably cheap by the 1890s. They cost about one day's wages. Suddenly everybody has access to accurate timekeeping all the time, and that's a really interesting change. When people write about clocks in history, they use the same word for a bunch of different things. There's a famous example: there was a fire in a particular monastery, and the record says some of the brothers ran to the well, and some ran to the clock. That tells you that it was a water clock because they're going there to fill up buckets to put the fire out. There's another reference that says a clock was installed above the rood screen of a church. If it's 50 feet up in the air, that was probably not a water clock but a mechanical clock because nobody would make a device where you have to go up there and fill it with water. I really like this Chinese tower clock. It was built in about A.D. 1100 by a court official, Su Song. It's a water clock, based on a constant flow of water, but it's a mechanical device. It's this giant wheel that has buckets at the end of arms, and the bucket is positioned under this constant-flow water source. When it fills past a certain point, the bucket tips, and that releases a mechanism that lets the wheel rotate. The wheel turns and brings a new bucket that starts filling. The timing regulation is really provided by the water, which also provides the drive force. The weight of the water is what's turning the wheel. It's this weird hybrid between an old-school water clock and the mechanical clocks that would be developed in Europe a century or two later. It's an amazingly intricate system, a monumental thing that worked incredibly well. But it didn't last long-it ran for about 20 years. It was located in a capital city of a particular dynasty, which fell, and the successors couldn't make it work. It's a neat episode in history. The origin of this is that Su Song was sent to offer greetings on the winter solstice to a neighboring kingdom. But the calendar was off by a day. He got there and gave his greeting on the wrong day, which would have been a huge embarrassment. When he got back, the calendar makers were punished, and he said, I'm fixing this. Every civilization that we have decent records of has its own way of keeping time. It's very interesting because there are all these different approaches. None of the natural cycles you see are commensurate with one another. A year is not an integer number of days, and it's not an integer number of cycles of the moon. So you have to decide what you're prioritizing over what else. You have systems such as the Islamic calendar, which is strictly lunar. They end up with a calendar that is 12 lunar months, which is short [compared with about 365 days in a solar year], so the dates of the holidays move relative to the seasons. The Jewish calendar is doing complicated things because they want to keep both: they want holidays to be associated with seasons, so they have to fall in the right part of the year, but they also want them in the right phase of the moon. The Gregorian calendar sort of splits the difference: We have months whose lengths are sort of based on the moon, but we fix the months, so the solstice is always going to be June 20, 21 or 22. We give the position of the year relative to the seasons priority above everything else. Then you have the Maya doing something completely different. Their calendar involves this 260-day interval, and no one is completely sure why 260 days was so important. The official time now is based on cesium: one second is 9,192,631,770 oscillations of the light emitted as cesium moves between two particular states. I've taught this a bunch of times [laughs]. Time is defined in terms of cesium atoms, so the best clocks in the world are cesium clocks. Cesium clocks are good to a part in 1016. If it says one second, there are 15 zeros after the decimal point before you get to the first uncertain digit. There are experimental clocks that are two, maybe three, orders of magnitude better than that. They are not officially clocks. They are measuring a frequency and measuring it to a better precision than the best cesium clocks. Those are good enough that an aluminum ion clock did a test of relativity. [Researchers] held the ion fixed in one, and the other, they shook back and forth, and they could see that the one that was moving ticked a little slower. Then they held one in position and moved one about a foot higher, and they could see that the one at higher altitude ticked faster. They agreed perfectly with relativity. There's a limit in the sense that there are a lot of things that affect the precision. Einstein's general theory of relatively tells you that the closer you are to a large mass, the slower your clock will tick. At some point, you're sensitive to the gravitational attraction of graduate students coming into and out of the lab. At that point, it becomes impractical. This is already an issue because the atomic time for the world is a consensus of atomic clocks all over the world. Here in the U.S., there are two big standards labs: one is the Naval Observatory in Washington D.C., around sea level, and the other is in Boulder, Colo., about a mile up. Their cesium clocks tick at different rates because they're at different distances from the center of the earth. They have to take that into account. So we kind of made an unwise choice locating this in Boulder. I have two: One is a quartz watch-nothing special. I've probably spent more replacing the band on it several times than I spent on the watch. The other I have is a mechanical watch from the 1960s. It's an Omega, purely mechanical watch. It's an incredibly intricate watch, a marvel of engineering, but I can go to a dollar store and buy a watch that can keep time just as well because quartz is so accurate. The pathway to humans on Mars lies through the atom, split. Far from Earth, whether in the void or on another world, power is life. A steady, strong flow of electricity is as crucial for operating computers and engines as it is for assuring access to corporeal necessities such as light and heat, breathable air and potable water, and preparation or even growth of food. And one of the most potent and reliable ways to get all those vital kilowatts is via nuclear fission-something aspiring astronauts realized long before anyone ever reached space (or developed nuclear weapons, for that matter). Yet more than 60 years into the space age, nuclear fission for spaceflight remains mostly a dream. Now, however, as NASA pursues its Apollo-esque Artemis program to build a crewed lunar outpost (with an eye toward eventual human landings on Mars), a rare alignment of technology, funding and political will is on the verge of making spaceborne nuclear reactors a routine reality. In 2020 the White House gave NASA a 10-year deadline to deliver a 10-kilowatt nuclear power system to the surface of the moon. The project is now a top priority of the agency's Space Technology Mission Directorate. And in July 2021 congressional appropriators earmarked $110 million for NASA to advance development of a new nuclear rocket suitable for sending cargo and crew on interplanetary voyages. NASA had not even asked for the money. The reason for this sudden urgency is simple: Without nuclear power, the space agency's stated goal of establishing a moon base by the end of the decade-let alone putting boots on Mars-becomes difficult, if not impossible, to achieve. Surprisingly, no fundamental technology breakthroughs are required to build a nuclear reactor for spaceflight applications. In fact, the U.S. already did so once-and so far only once-with the Air Force's development and launch of a working prototype in 1965. Instead the difficulty lies in navigating the complex web of regulations that surrounds all things nuclear and in ensuring any chosen approach for nuclear power beyond Earth does not needlessly limit NASA to just the lunar surface or any other lone deep-space destination. Ideally, the power of the atom can be harnessed not only for crewed missions to the moon and Mars but also for robotic exploration throughout the solar system. The goal going in is make sure that what we use on the moon from a fission reactor standpoint is also directly applicable for use on the surface of Mars, says Michael Houts, manager of nuclear research at NASA's Marshall Space Flight Center. Fission, he explains, is a pretty simple process. It's literally just the right materials in the right geometry, Houts says. That's why, once it was discovered, we very quickly had systems able to self-sustain a chain reaction. This differs completely from the radioisotope thermoelectric generators (RTGs) that power NASA's Mars rovers, the New Horizons mission to Pluto and beyond, and the Voyager spacecraft now in interstellar space. RTGs merely convert the heat released from naturally decaying plutonium into electricity. Fission reactors are far more powerful and versatile, splitting atoms from uranium fuel and channeling the released energy into propulsion and electricity production. There are no physics breakthroughs needed, no miracles necessary. But just like terrestrial systems, you're going to need to have some really good engineering, Houts says. NASA is publicly cagey about its Mars timeline, but since the first term of former president George W. Bush, the agency has steadily worked toward a giant leap on the Martian surface by the end of the 2030s. In 2020 NASA asked the National Academies of Sciences, Engineering, and Medicine to study the technical challenges, benefits and risks of nuclear propulsion, with particular emphasis on a notional nuclear-propelled cargo launch to Mars in 2033 that would precede a human mission in 2039. In logistic terms, what such a mission would look like has scarcely changed since the 1950s. Three years before Yuri Gagarin's flight made humans a spacefaring species, NASA's precursor, the National Advisory Committee for Aeronautics, began a formal study of nuclear propulsion as part of a crewed Mars expedition. This investigation called for a 420-day expedition with 40 days at Mars. Other, more ambitious proposals have examined lengthier surface sojourns on Mars stretching to around 500 days, but the classic mission profile has remained the dominant vision for crewed Mars exploration, driven in part by celestial mechanics and reasons of survival: To conserve fuel, both Earth and Mars must be properly aligned in their orbit. And technologically speaking, humans are not yet ready to cut the terrestrial umbilical cord and truly live off the land in space. The human body can handle the journey, as evidenced by decades of data from crews living and working on space stations in low-Earth orbit. The current record for the longest continuous stay in space is held by the cosmonaut Valeri Polyakov. Thanks to a vigorous off-world workout regimen, he was able to walk from his capsule after landing despite having spent 437 days in muscle-wasting microgravity onboard the Soviet space station Mir. Upon returning to Earth, Polyakov's first words to a fellow cosmonaut reportedly were We can fly to Mars. NASA's current goal for a Mars mission calls for a round trip of about two years. Nuclear propulsion would be a critical enabler. In addition to increasing the number of flight opportunities for a crewed mission, it would reduce the number of flights necessary to get the fuel for such a trip into Earth's orbit. Those fuel requirements are considerable. The International Space Station, painstakingly built via more than three dozen launches across a decade's time, is approximately 420 metric tons. A chemical propulsion system necessary for a round trip to Mars would require the very expensive task of lofting somewhere between more than twice to nearly 10 times as much tonnage from Earth. Consider that the mightiest of NASA's rockets-the Space Launch System (SLS), which has yet to even fly-is slated to carry a mere 95 metric tons to space at $2 billion per launch. If-or whenthe SLS is superseded by more capable and cost-effective rockets such as SpaceX's in-development and all-reusable Starship, that single-launch mass limit will increase to more than 100 metric tons, and the price per launch should plummet. Even so, the financial calculus of a chemically fueled Mars mission would still be daunting. In contrast, an analogous Mars mission using nuclear propulsion would require sending up a total mass of between 500 and 1,000 metric tons. Launching the equivalent of a single space station-maybe two-is plausible. After all, we have done it before. NASA is presently pursuing not one but two classes of atomic-powered rocketry: nuclear thermal propulsion and nuclear electric propulsion. Either of these approaches could pair with nuclear surface power-the third key fission technology under study by the space agency. Nuclear thermal propulsion implemented on the interplanetary scale would essentially be a ferry or transfer stage-a smaller nuclear-powered rocket that would dock with other transport elements in orbit before pushing its separately launched payload onward. Such an arrangement operates much like a chemical propulsion system, although the combustion chamber-where a rocket's fuel and oxidizer mix and ignite, producing hot exhaust forced from the rocket nozzle-is replaced with a nuclear reactor that heats a cryogenic propellant, blasting it through the nozzle to generate thrust. The process, viewed externally, looks virtually identical: a rocket engine blasting fire. Nuclear electric propulsion, on the other hand, works a lot like a nuclear power plant on Earth, in which fission reactions are used (via an intermediate step such as driving a turbine) to generate electricity. That electricity, in turn, can power an electric propulsion system similar to (but far stronger than) the solar-powered ion thrusters on NASA's Dawn, a spacecraft that explored the asteroid Vesta and dwarf planet Ceres. There are trade-offs to each approach. The greatest challenge of nuclear thermal propulsion is that it is a high-performance reactor operating at a high temperature, reaching circa 2,500 degrees Celsius-an unnerving prospect for astronauts and materials engineers. The reactor would also require immense volumes of cryogenic propellant, likely sourced from on-orbit storage tanks that carry major engineering challenges of their own. But the approach's focused intensity has an upside: The propulsion system only needs to run for a few hours total, Houts says. You get all your [work] done very quickly. After that, the spacecraft has all the speed it needs for a trip to Mars or home. Nuclear electric propulsion, meanwhile, runs at lower temperatures and power levels, but it must operate continuously for months or even years, building fantastic speeds over time. It is a more complex system than its thermal counterpart in many ways. And it is less developed: the calculated performance levels for near-term designs are far below what would be necessary for a crewed mission to Mars. The power produced by a nuclear electric propulsion system's reactor must be converted multiple times (rather than just being absorbed and dissipated by propellant blown out the back of a rocket). Conversions can only be done with efficiency percentages ranging from the mid-30s to 40. The rest of that thermal energy must somehow be dealt with: present concepts call for massive radiators to dissipate the excess heat into space. The nuclear electric spacecraft would also require a short, sharp kick from an old-fashioned chemical propulsion system to help it escape Earth's orbit and another to enter and depart orbit around Mars. In part because of its relative simplicity, nuclear thermal propulsion is the clear favorite among Mars mission planners-and U.S. politicians. This was the approach that netted the $110-million endorsement of congressional appropriators in July 2021 and that the NASA-sponsored National Academies report flagged as most plausible for enabling a 2039 crewed mission to the Red Planet. Nuclear thermal propulsion also has the advantage of a rich inheritance: The U.S. government-chiefly the Department of Defense-has been fitfully trying to get the technology flying since the dawn of the space age. One bold early attempt traces to a 1955 Air Force effort known as Project Rover, which sought to build a nuclear thermal upper stage for intercontinental ballistic missiles. But chemical propulsion soon proved sufficient for that job, so Rover was absorbed into NASA, where it became the Nuclear Engine for Rocket Vehicle Application (NERVA) program. In the late 1950s, the DoD started work on the Systems for Nuclear Auxiliary Power (SNAP) program, an effort to launch space nuclear reactors to power long-duration missions such as spy satellites. Both projects achieved impressive results. SNAP led to the Air Force's 1965 launch of SNAP-10A, the only U.S. fission reactor ever sent to space. The reactor functioned for six weeks in orbit. NERVA, meanwhile, successfully developed and tested nuclear thermal rockets on Earth. And the program was, for a time, central to NASA's post-Apollo plans for Mars exploration. But the Nixon administration instead chose to pursue the space shuttle and canceled both projects in 1973. NERVA was briefly resurrected in the late 1980s by an Air Force–led effort, the Space Nuclear Thermal Propulsion program, but by the early 1990s interest had fizzled again. Nuclear electric propulsion, too, had its brief moment in NASA's limelight. In 2003 an initiative called Project Prometheus brought together NASA, the U.S. Navy's submarine reactor program and the Department of Energy-this time to build a nuclear electric propulsion fleet for science missions. Spaceborne fission would enable a single spacecraft to explore multiple targets in the outer solar system and even beyond, where sparse sunlight profoundly limits solar power's potential. Project Prometheus would have been nothing short of revolutionary: its reactor would have produced 200,000 watts of power for a spacecraft's propulsion and instruments. By comparison, the New Horizons probe operates on just 200 watts of power-that is, about two or three incandescent light bulbs' worth. NASA, however, snuffed out Prometheus after two years, citing budget concerns. One might think all these past projects would be a huge boost for today's push to develop atomic-powered rocketry, but their mercurial nature makes them of limited use. Historically, if you spend three or four years developing a nuclear propulsion system, and then you stop, and you come back a decade later, you've got to recapture a lot of knowledge, says Shannon Bragg-Sitton, a leading nuclear engineer at the Idaho National Laboratory and co-author of the National Academies report. The fact that we've been looking at both these systems since the 1950s doesn't mean that we have 70 years of knowledge. It means that we started thinking about them then, and we made some efforts in each of them. NASA's notional target date of 2039 for a crewed Mars mission might seem so far off that urgent action is not yet necessary, but Bragg-Sitton says the timing is deceptive. The tentative plan calls for nuclear-powered cargo flights to begin six years earlier, in 2033, to preposition materials on Mars and serve as dry runs for crewed transport. We need to be ready to actually launch our first system for qualification with those supply missions, she says. Well, now the timeline is not as long as it sounded initially! Ideally, she says, hardware designs for a flight in 2033 would be locked-in by 2027. That means the time is now to make critical decisions, chief among them comparing and choosing between nuclear thermal and nuclear electric propulsion. You can't develop a nuclear system in a year or two-it's just the way it is, Bragg-Sitton concludes. None of this is out of our reach. It just takes a lot of focus to get it done. But first, someone needs to let them do it. Getting approval to launch nuclear materials into space, it turns out, is at least as challenging as actually building a space-ready nuclear reactor or rocket. This is especially true if your fission system relies on highly enriched uranium-that is, uranium composed of 20 percent or more of the fissile isotope uranium 235. Only 1 percent of Earth's naturally occurring uranium takes this form, which is prized by warhead designers and spacecraft engineers striving to make their creations as featherweight and powerful as possible. The more uranium 235 your nuclear fuel has, the smaller you can make your reactor-or your bomb, which is why the material is subject to such strict regulations. For NASA, even a nuclear payload without highly enriched uranium has enormous hurdles to clear-namely a labyrinthine safety analysis process that often involves many other federal agencies and culminates in NASA's administrator approving or rejecting a launch. If a rocket carries highly enriched uranium, however, it can only be launched after formal authorization from the White House. The additional stringency associated with this highest tier of approval can easily add several years and tens of millions of dollars to a project's schedule and budget. Find a way to avoid using highly enriched uranium, then, and you may secure a far faster and cheaper path to the launchpad. There are, in fact, new designs for advanced high-power reactors that use large amounts of low-enriched uranium rather than small amounts of highly enriched material. But whether or not NASA ultimately pursues such an approach for its nuclear aspirations may be dictated by the work of another federal entity: the Defense Advanced Research Projects Agency wants to launch one of these new reactors to space by 2025 to power a proof-of-concept nuclear propulsion system-a timeline that would be aggressive even by Apollo standards. DARPA calls the system the Demonstration Rocket for Agile Cislunar Operations, or DRACO. The program's murky origins involve DoD demands for some of its classified missions to have the capability to maneuver in space faster than would be possible through chemical propulsion. DARPA's gamble with DRACO is twofold: it seeks to reach the launchpad quickly by using a new type of reactor and by minimizing Earth-based trials, thus bypassing the presidential-tier launch approval process and a rat's nest of ground-testing red tape. This bold strategy arose from the agency's judgment that such tests are now virtually impossible to perform because of prohibitive regulations and inadequate infrastructure. One cannot, for instance, simply update and use the specialized facilities that supported NERVA testing-they were razed when the program ended. Building new test facilities is undesirable, too, because doing so would require billions of dollars and several years of work during which the project could easily be scuttled by shifting political priorities. Although DARPA's accelerated plan calls for robust ground testing of DRACO's smaller components, this does not include operating the full reactor at full power. Astoundingly, the very first time DRACO's reactor would turn on would be in space. Starting the reactor is going to be entirely based on our predictions, says Tabitha Dodson, a project manager for DRACO at DARPA. We are going to put a lot of guesswork into our modeling and simulations before launching the engine, without ever having tested it on the ground. Data from the NERVA tests of yore should help, Dodson says, but the task before the DRACO team remains extremely challenging. After more than a half-century of starts and stops, says Air Force major Nathan Greiner, another DARPA project manager, launching a nuclear reactor would be a critical enabler. Let's get this all the way across the finish line-not just small elements, not just a reactor on the ground, but, no kidding, let's go build a spacecraft and put it in space, he says. Such an existence proof would then ease the way for NASA or the DoD in any future overtures to congressional appropriators. The question would no longer be Does this technology exist? but rather Do you want more of it-or not. Of course, DARPA alone cannot spark a spaceflight revolution. Nuclear propulsion for space exploration is a whole-of-government effort. At minimum, the Department of Energy will need to make more low-enriched uranium. One agency or another-most likely, several working together-will have to develop orbital fuel depots to provide outbound missions with cryogenic propellants and will have to find better, safer ways to perform ground tests of interplanetary-scale propulsion systems. And then NASA must actually build the rockets. DRACO will not get NASA and its astronauts all the way to Mars, Greiner says, but this is going to take it a hell of a long way along that path. If nothing else, today's push for nuclear power in space is a useful metric for measuring the seriousness of NASA's-and the nation's-lunar and Martian ambitions. In the context of human spaceflight, NASA has a well-known aversion to new (and thus presumably more risky) technology-but in this case, the old way makes an already perilous human endeavor needlessly difficult. For all the challenges of embracing nuclear power for pushing the horizon outward for humans in space, it is hard to make the case that tried-and-true chemical propulsion is easier or carries significantly less physical-and political-risk. Launching 10 International Space Stations worth of mass across 27 superheavy rocket launches for fuel alone for a single Mars mission would be a difficult pace for NASA to sustain. That is more than 40 launches and at least 80 billion if the agency relies on the SLS. And such a scenario assumes everything goes perfectly: sending help to a troubled crew on or around Mars would require dozens of additional fuel launches, and chemical propulsion allows very limited windows of opportunity for the liftoff of any rescue mission. If, with a single technology, that alarmingly high number of ludicrously expensive launches could be cut down to three-while also offering more chances to travel to Mars and back-how could a space agency that was earnest in its ambitions not pursue that approach. No miracles are necessary, and regulators and appropriators seem to agree that the time has come. As Polyakov said, We can fly to Mars. Splitting atoms, it seems, is now the safest way to make that happen. As the Omicron variant continues to spread wildly, millions of people are worried they have been exposed-and are desperate to find out if they are infected. But in many areas, at-home rapid antigen tests are difficult to find, and PCR tests-if an appointment is available-can take days to return a result. So if you can get your hands on a precious test, when is the best time to use it for the most accurate result. With the original strain of SARS-CoV-2, the coronavirus that causes COVID, the amount of virus in the body-known as viral load-generally reaches its peak five to seven days after exposure, says Gigi Kwik Gronvall, an immunologist at the Johns Hopkins Bloomberg School of Public Health. The Centers for Disease Control and Prevention and many state health departments recommend that exposed people who have been vaccinated wait until this time to take a test after contact with someone who has COVID to be certain that a negative test is truly negative because the highest viral load will provide the best chance of detecting the virus. But with the Omicron variant, that may no longer be the case: symptoms seem to appear sooner after infection. As a result, infected people could spread the virus to others sooner as well. Recent studies suggest that people infected with Omicron may continue to shed the virus for many days after the onset of symptoms, meaning that the current CDC recommendation to isolate for five days after infection may not be long enough. The two main types of COVID tests available to consumers are vastly different in their sensitivity. A PCR test, which measures viral RNA in the body, can detect as few as 200 copies of SARS-CoV-2 per milliliter of sample. A rapid antigen test, by contrast, may not come back positive if there are fewer than 500,000 viral copies. It is unclear whether a higher viral load necessarily means a person is more infectious-after all, the amount of virus replicating in the body does not necessarily track with the number of viral particles that person emits when they breathe. But Gronvall says that the assumption is that a positive rapid antigen test is a good marker for contagiousness because these tests do not detect SARS-CoV-2 until peak viral load. A PCR test is so sensitive that it may continue to show the virus in the body long after a person is no longer contagious. At this point in the pandemic, it has become more difficult for epidemiologists to say with certainty whether one variant reaches a higher viral load or how that viral load correlates with infectiousness, notes Ajay Sethi, an epidemiologist at the University of Wisconsin–Madison. That is because so many people have now been infected with COVID or received different numbers of vaccine doses, meaning that their immune systems respond differently to the newer variant. It's too complicated to say one variant will produce a higher viral load, Sethi says. The guidance is based on the average time between infection and emergence of symptoms with the original strain of SARS-CoV-2: If symptoms do not emerge after five days, there is a very low chance that a person is infectious. The theory is that viral load and symptoms peak around the same time, providing enough virus for a PCR or rapid antigen test to definitively give a positive or negative result. But the Omicron variant's symptoms seem to come much earlier: three days after exposure on average. Furthermore, emerging evidence suggests that people with Omicron can become contagious at least a day before symptoms appear. A January 5 preprint paper posted on medRxiv tracked 30 people during an Omicron outbreak and found that most were infectious several days before rapid antigen tests detected COVID. Further confusing matters is a recent study of patients in Japan that found the Omicron variant might not reach peak viral load until three to six days after the onset of symptoms. But that does not mean people are not infectious much earlier, says Omai Garner, a clinical microbiologist at the University of California, Los Angeles, Health System. Because PCR tests can detect an infection before viral load peaks, they might be better at spotting Omicron right after a person acquires the variant but before they can spread the infection. Right now, however, it can take several days for PCR tests to return results, by which point the person will be capable of infecting others. There is no harm in testing two days after exposure. But the concern is that a negative antigen test at two days could provide a false sense of security because the viral load may still be too low at this point to be detected by rapid antigen tests. A negative test one or two days after a person is exposed-particularly a rapid antigen test-will be meaningless, and that person will need to test again before coming out of isolation. They can't assume they don't have COVID, Garner says. They need to act like they have COVID until they get a result that can affirm that they don't. The current shortage of tests complicates matters because it could discourage people from seeking such additional testing. It speaks to a lack of federal response to this virus, Garner says. Every time cases surge, we can't get enough testing. Data on the original coronavirus strain have shown that most people with mild COVID are no longer infectious after 10 days following symptoms. Garner says that the virus may act differently in people who are vaccinated, unvaccinated or only recently vaccinated, but there is little information on those differences at the moment. On December 27, 2021, the CDC changed its isolation recommendations to suggest that people with mild symptoms of COVID isolate for five days and then return to work with a mask, even if they have not received a negative test. The agency suggested that this guidance was based on new science, although it has not released the data, and many anecdotal cases suggest that Omicron infection can last longer than five days. And many experts believe that eliminating the testing requirement has more to do with avoiding test shortages than it does with new science, especially because so little is still known about Omicron. We put all this effort into discovering infectious people... why not at LEAST ensure those we do discover don't spread on, wrote infectious disease expert Michael Mina in a Twitter thread in which he called the CDC decision reckless. Initial evidence suggests that Omicron may be more prevalent in the throat than previous variants, which tend to replicate in the nose. Small initial studies have found that both PCR and antigen tests using saliva swabs produce positive results days earlier than nasal swabs. Numerous anecdotal reports also suggest that throat swabs produce more accurate results, although one study suggests that rapid antigen tests are less sensitive when used in the throat instead of the nose, as is recommended. Although a few experts have advocated for the collection of saliva, the Food and Drug Administration recommends against at-home throat swabs: In a January 7 Twitter thread, the agency said that, in addition to safety concerns, we don't have any data yet suggesting throat swabs are an accurate or appropriate method for at-home tests. Changing this guidance would require more research and the standardization of new types of tests, Garner says. Taking a PCR test at three days can tell you whether you have the virus, even if you are not contagious yet. If PCR tests are not available, and you only have access to one antigen test, your timing largely depends on the situation. If you have the ability to stay isolated and know exactly when you were exposed, Sethi and Garner agree that waiting five days to take an antigen test is your best bet to ensure that a negative test is actually negative or whether the symptoms you are experiencing are caused by COVID. But if you are asymptomatic, need to be in contact with people and have only one rapid antigen test, taking it at two days might be worthwhile. If it is positive, Sethi says, you can assume that you have COVID. If it's negative, you're going to wish you had another test, he adds. Either way, experts say you should act as if you have COVID until enough time has passed to know that the test is truly negative. A negative test-particularly a rapid antigen test-does not mean you are not infectious or will not become so later on. If I've said one thing over and over, it's that testing is a moment in time, Gronvall says. Until very recently, if there was one silver lining to the pandemic, it was that kids seemed to escape the worst of the virus. Very few became seriously ill or even mildly sick, compared with adults. But now that hopeful aspect may be fading. The number of children hospitalized with COVID has skyrocketed in recent weeks as the Omicron variant fueled a surge of infections, raising concerns that the latest version of the coronavirus may pose a greater threat to children. Nationwide, an average of 881 children under age 17 are being admitted to hospitals with COVID each day, according to the most recent data from the Centers for Disease Control and Prevention. Hospitalizations of children under the age of five, who are not eligible for the COVID vaccine, have soared to levels two to four times that of previous peaks. Experts believe the jump in pediatric hospitalizations is likely the result of a confluence of factors. One of them is Omicron's more contagious nature, and another may be the variant's newfound preference for airway passages above the lungs, which can be more easily blocked in small children. The big issue is that Omicron is infecting a lot more people. We refer to this as the denominator phenomenon, says Susan Coffin, an infectious disease specialist at Children's Hospital of Philadelphia. The hospitalization rate is calculated by dividing the number of hospitalizations-the numerator-by the number of known cases-the denominator. If the denominator becomes a bigger number, so will the numerator, the thinking goes. And indeed, a recent report from the American Academy of Pediatrics indicates that the denominator-specifically, the number of pediatric cases-is growing at an enormous rate. Out of the nearly 9.5 million children who have tested positive for COVID-19 since the beginning of the pandemic, nearly 20 percent of these cases occurred in just the first two weeks of January. Thus far, there are no signs that the cases caused by Omicron are more severe. If anything, preliminary evidence suggests the opposite may be true. Rong Xu, a data scientist at Case Western Reserve University, analyzed health records from nearly 80,000 children under age five who developed COVID-19 before and after the emergence of Omicron. She found that the risk of hospitalization in those who became sick when that variant was dominant was one third of what was observed when Delta reigned supreme (1 percent versus 3 percent). The study is consistent with previous work showing similar trends for children of all ages and is supported by a recent analysis of COVID patients in California. So why the jump in hospitalized children. The risk of hospitalization is not zero, Xu says. So if you multiply it by a big number-if more kids are getting infected-you are going to see a lot more kids in the hospital. Though scientists are still teasing apart the properties of Omicron that enable it spread so rapidly among children and adults alike, one aspect that could give the variant a leg up is its ability to evade the immune response. In the first waves of the pandemic, kids fared better than adults in large part because children have more robust innate immune systems, which mount rapid initial responses to invading microbes. Adults, in contrast, have better adaptive immune systems, which respond effectively after an infection has begun to take hold in the body. Everybody started out with a clean slate, having never seen the virus, says Betsy Herold, a pediatric infectious disease physician at the Albert Einstein College of Medicine. In a study published in 2020, Herold showed that children mounted a swift innate immune response to COVID, churning out potent antiviral proteins known as interferons and interleukins that quashed the coronavirus early in infection. But two years in, and a Greek alphabet of viral variants later, circumstances have changed. Researchers at the Quantitative Biosciences Institute at the University of California, San Francisco, and their colleagues recently reported that SARS-CoV-2 has picked up mutations that weaken the innate immune response. Molecular biologist Mehdi Bouhaddou, who co-led the research, says that while he and his coworkers experiments focused on the Alpha variant, the Delta and Omicron variants also carry the same immunity-disabling mutations. But because those mutations appeared in all three variants, they alone cannot explain fluctuations in pediatric hospitalizations. Both Bouhaddou and Herold are launching studies to investigate whether Omicron has a unique impact on innate immunity. My hunch is that the innate immune system is still strong, Herold says. But it's a balance, right. If you have a ton of virus, then no matter how good your innate response is, some of that virus is going to win out. And with Omicron, there is a ton of virus. This variant replicates 70 times faster than Delta in human airways, according to preliminary data shared in a news release from the University of Hong Kong. That same research and a slew of animal studies show that the variant has a harder time multiplying in lung tissue, suggesting why it might cause less severe disease. This evolutionary journey into a more transmissible but milder version is somewhat expected, though in no way guaranteed, Bouhaddou says. Viruses typically evolve to become less dangerous over time, he adds. Still, Omicron's predilection for the respiratory tract above the lungs could spell trouble for the youngest children, whose airways are narrower and less developed. Coffin says it is easier for these tiny airways to be obstructed by mucus and inflammation, causing infants and toddlers to develop wheezing or croup, a disease known for its characteristic barking cough. These are classic syndromes of childhood, and we are pretty adept at taking care of them, she says. Though these conditions can land children in the hospital, they are easily treatable, regardless of whether they are caused by SARS-CoV-2 or another virus. In some parts of the country, pediatric hospitalizations are now starting to edge downward as case counts begin to ease. Experts recommend that families trying to weather this wave continue to do what they can to keep kids safe: getting vaccinations and boosters when possible, wearing masks and avoiding social activities at the first sign of symptoms. These are all things that have been proven time and time again over the past 20 months to work, Coffin says. And they work really well against Omicron, too. More valuable than silver, gold, platinum, or even this jewelry. It's this powder right here. This jewelry is being treated with a thin coating of rhodium-a chemically inert, corrosion resistant metal. It protects the silver and gives it a nice shiny finish. But you probably use rhodium, every single day, for another reason. Rhodium is a key ingredient in every car sold in the United States since around 1975. It's part of a system that cleans out pollutants and prevents them from entering the atmosphere. And it's also the reason why thieves across the U.S are sawing off catalytic converters in order to get their hands on a few precious grams of the world's most valuable metal. So how did we get here? Well, in the 1970s, air in the United States was getting dirty. And a big reason was automobiles. Massive public pressure caused The U.S. Congress to pass the Clean Air Act of 1970, setting national standards for air quality. One of the goals of these standards was reducing harmful emissions by automobiles, specifically a 90 percent reduction in automobile emissions from pre-1968 levels by the 1975 model year. Engineers and researchers at Engelhard Industries and Corning Glassworks ultimately developed what is the modern three-way catalytic converter. The converter itself sits just behind the exhaust manifold and before the muffler. It's purpose: to reduce 3 harmful types of emission: carbon monoxide, hydrocarbons (or unburned fuel), and nitrous oxides. The converter on regular fuel vehicles is simple: a stainless steel shell surrounds a ceramic honeycomb monolith that monolith is coated with three important precious metals: platinum, palladium, and rhodium. As the car's exhaust passes through this honeycomb the metals heat up and act as catalysts: turning carbon monoxide into carbon dioxide, unburned hydrocarbons to H20 and C02, and nitrous oxides into nitrogen and Carbon-dioxide. Because these metals, and especially rhodium are so stable and durable they can perform this function over an extremely long lifetime of the car part-suffering very little loss in performance. But as effective as the Rhodium is at catalyzing car exhaust they have a major drawback: cost. These are called precious metals for a reason. Morris Bullock: Precious metals are those that are very expensive and they're expensive because they're rare - that is they have a low Earth abundance. So precious metals include the platinum group metals which are ruthenium, rhodium, palladium, platinum, osmium and iridium but also precious metals include other metals that are more familiar to the public like gold and silver. Just to give you an example, rhodium which is one of the metals used in catalytic converters, the earth abundance of that is on the order of 1 part per billion. Iron for example Earth abundance is about 5 percent. Robinson: So if rhodium has always been rare, why are prices spiking now. Closed mines and refineries created a huge deficit in rhodium, palladium, and platinum supply even as demand was increasing around the world. It's a perfect storm creating a huge spike in prices for all of these precious metals and especially rhodium. Because of this deficit, rhodium prices peaked In March of 2021. And if car manufacturers can't buy these metals from mines-they'll get it somewhere else: recycling.